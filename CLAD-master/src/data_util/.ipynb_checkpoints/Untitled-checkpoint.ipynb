{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387db6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26352d15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c15d1bc368f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "datasets.logging.set_verbosity(datasets.logging.ERROR)\n",
    "\n",
    "task_to_keys = {\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    '20ng': (\"text\", None),\n",
    "    'trec': (\"text\", None),\n",
    "    'imdb': (\"text\", None),\n",
    "    'wmt16': (\"en\", None),\n",
    "    'multi30k': (\"text\", None),\n",
    "}\n",
    "\n",
    "\n",
    "def load(task_name, tokenizer, max_seq_length=256, is_id=False):\n",
    "    sentence1_key, sentence2_key = task_to_keys[task_name]\n",
    "    print(\"Loading {}\".format(task_name))\n",
    "    if task_name in ('mnli', 'rte'):\n",
    "        datasets = load_glue(task_name)\n",
    "    elif task_name == 'sst2':\n",
    "        datasets = load_sst2()\n",
    "    elif task_name == '20ng':\n",
    "        datasets = load_20ng()\n",
    "    elif task_name == 'trec':\n",
    "        datasets = load_trec()\n",
    "    elif task_name == 'imdb':\n",
    "        datasets = load_imdb()\n",
    "    elif task_name == 'wmt16':\n",
    "        datasets = load_wmt16()\n",
    "    elif task_name == 'multi30k':\n",
    "        datasets = load_multi30k()\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key] + \" \" + examples[sentence2_key],)\n",
    "        )\n",
    "        result = tokenizer(*inputs, max_length=max_seq_length, truncation=True)\n",
    "        result[\"labels\"] = examples[\"label\"] if 'label' in examples else 0\n",
    "        return result\n",
    "\n",
    "    train_dataset = list(map(preprocess_function, datasets['train'])) if 'train' in datasets and is_id else None\n",
    "    dev_dataset = list(map(preprocess_function, datasets['validation'])) if 'validation' in datasets and is_id else None\n",
    "    test_dataset = list(map(preprocess_function, datasets['test'])) if 'test' in datasets else None\n",
    "    return train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "\n",
    "def load_glue(task):\n",
    "    datasets = load_dataset(\"glue\", task)\n",
    "    if task == 'mnli':\n",
    "        test_dataset = [d for d in datasets['test_matched']] + [d for d in datasets['test_mismatched']]\n",
    "        datasets['test'] = test_dataset\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_20ng():\n",
    "    all_subsets = ('18828_alt.atheism', '18828_comp.graphics', '18828_comp.os.ms-windows.misc', '18828_comp.sys.ibm.pc.hardware', '18828_comp.sys.mac.hardware', '18828_comp.windows.x', '18828_misc.forsale', '18828_rec.autos', '18828_rec.motorcycles', '18828_rec.sport.baseball', '18828_rec.sport.hockey', '18828_sci.crypt', '18828_sci.electronics', '18828_sci.med', '18828_sci.space', '18828_soc.religion.christian', '18828_talk.politics.guns', '18828_talk.politics.mideast', '18828_talk.politics.misc', '18828_talk.religion.misc')\n",
    "    train_dataset = []\n",
    "    dev_dataset = []\n",
    "    test_dataset = []\n",
    "    for i, subset in enumerate(all_subsets):\n",
    "        dataset = load_dataset('newsgroup', subset)['train']\n",
    "        examples = [{'text': d['text'], 'label': i} for d in dataset]\n",
    "        random.shuffle(examples)\n",
    "        num_train = int(0.8 * len(examples))\n",
    "        num_dev = int(0.1 * len(examples))\n",
    "        train_dataset += examples[:num_train]\n",
    "        dev_dataset += examples[num_train: num_train + num_dev]\n",
    "        test_dataset += examples[num_train + num_dev:]\n",
    "    datasets = {'train': train_dataset, 'validation': dev_dataset, 'test': test_dataset}\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_trec():\n",
    "    datasets = load_dataset('trec')\n",
    "    train_dataset = datasets['train']\n",
    "    test_dataset = datasets['test']\n",
    "    idxs = list(range(len(train_dataset)))\n",
    "    random.shuffle(idxs)\n",
    "    num_reserve = int(len(train_dataset) * 0.1)\n",
    "    dev_dataset = [{'text': train_dataset[i]['text'], 'label': train_dataset[i]['label-coarse']} for i in idxs[-num_reserve:]]\n",
    "    train_dataset = [{'text': train_dataset[i]['text'], 'label': train_dataset[i]['label-coarse']} for i in idxs[:-num_reserve]]\n",
    "    test_dataset = [{'text': d['text'], 'label': d['label-coarse']} for d in test_dataset]\n",
    "    datasets = {'train': train_dataset, 'validation': dev_dataset, 'test': test_dataset}\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_imdb():\n",
    "    datasets = load_dataset('imdb')\n",
    "    train_dataset = datasets['train']\n",
    "    idxs = list(range(len(train_dataset)))\n",
    "    random.shuffle(idxs)\n",
    "    num_reserve = int(len(train_dataset) * 0.1)\n",
    "    dev_dataset = [{'text': train_dataset[i]['text'], 'label': train_dataset[i]['label']} for i in idxs[-num_reserve:]]\n",
    "    train_dataset = [{'text': train_dataset[i]['text'], 'label': train_dataset[i]['label']} for i in idxs[:-num_reserve]]\n",
    "    test_dataset = datasets['test']\n",
    "    datasets = {'train': train_dataset, 'validation': dev_dataset, 'test': test_dataset}\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_wmt16():\n",
    "    datasets = load_dataset('wmt16', 'de-en')\n",
    "    test_dataset = [d['translation'] for d in datasets['test']]\n",
    "    datasets = {'test': test_dataset}\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_multi30k():\n",
    "    test_dataset = []\n",
    "    for file_name in ('./data/multi30k/test_2016_flickr.en', './data/multi30k/test_2017_mscoco.en', './data/multi30k/test_2018_flickr.en'):\n",
    "        with open(file_name, 'r') as fh:\n",
    "            for line in fh:\n",
    "                line = line.strip()\n",
    "                if len(line) > 0:\n",
    "                    example = {'text': line, 'label': 0}\n",
    "                    test_dataset.append(example)\n",
    "    datasets = {'test': test_dataset}\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def load_sst2():\n",
    "    def process(file_name):\n",
    "        examples = []\n",
    "        with open(file_name, 'r') as fh:\n",
    "            for line in fh:\n",
    "                splits = line.split()\n",
    "                label = splits[0]\n",
    "                text = \" \".join(splits[1:])\n",
    "                examples.append(\n",
    "                    {'sentence': text, 'label': int(label)}\n",
    "                )\n",
    "        return examples\n",
    "    datasets = load_dataset('glue', 'sst2')\n",
    "    train_dataset = datasets['train']\n",
    "    dev_dataset = datasets['validation']\n",
    "    test_dataset = process('./data/sst2/test.data')\n",
    "    datasets = {'train': train_dataset, 'validation': dev_dataset, 'test': test_dataset}\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3ab34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
